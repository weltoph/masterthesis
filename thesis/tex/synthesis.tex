\chapter{Synthesis}
\label{chapter:synthesis}
For the general synthesis question we refer to \cite{Church} and formalize it
with 
\begin{definition}[Synthesis Problem]
  Given a logical specification  $\phi(\cdot, \cdot)$ over inputs and 
  outputs from $I^{\omega}$ and $J^{\omega}$ respectively. The synthesis 
  problem requires to compute for any such $\phi$ an algorithm 
  $S:I^{+}\rightarrow J$ such that for every 
  $\alpha_{1}\alpha_{2}\dots\in I^{\omega}$ and every 
  $S(\alpha) = S(\alpha_{1})S(\alpha_{1}\alpha_{2})\dots$ satisfies 
  $\phi(I, S(I))$ or prove that such an $S$ cannot exist (cp. the 
  illustration in Figure \ref{fig:synthesis}).
\end{definition}
\begin{drawing}
  \caption{Illustration of the synthesis question. The aim is to provide an 
  algorithm which \enquote{synthesises} for any specification a strategy or 
  proves that there cannot exist a strategy that satisfies the specification.}
  \label{fig:synthesis}
  \begin{center}
    \includegraphics{tikz/synthesis.pdf}
  \end{center}
\end{drawing}
Naturally, the complexity of this problem is tightly related to the 
expressibility of $\phi$. The comprehensive demand of this question, namely
to generate for all inputs an associated \enquote{good} output, allows for a
game-theoretic formulation. As suggested in \cite{SeqCondStrat} we can consider
the environment as antagonistic and formulate the synthesis question in terms
of strategies, i.e. one player generates inputs while another player generates
outputs. The output-player wins if and only if $\phi$ is satisfied for the
input- and output-sequences. In the following we present known synthesis results 
for those $\phi$ which allow to capture the associated relation as a
$\omega$-regular language. Subsequently, we pivot towards probabilistic 
environments and review an approach to synthesis algorithms there which are not
necessarily completely correct but almost-surely satisfy a specification 
$\phi$. At last, we examine probabilistic environments which are modelled as
\acp{POMDP}. It is noteworthy that we consider environments by increasing 
domain knowledge.

\section{$\omega$-regular Languages}
In the following we consider an $\omega$-regular class of specifications 
$\phi$ for inputs $I$ and outputs $J$, namely those such that we can define a 
$\omega$-regular language $\mathcal{L}_{\phi}\subseteq 
\tuple{J\times I}^{\omega}$ such that
\begin{equation*}
  \tuple{j_{0}, i_{0}}\tuple{j_{1}, i_{1}}\dots\in\mathcal{L}_{\phi}
  \text{ iff }
  \phi(i_{0}i_{1}\dots, j_{1}j_{2}\dots)\text{ is true.}
\end{equation*}
Observably, the output symbol $j_{0}$ is irrelevant but included to ease the 
technicalities of the following argument. With the well researched theory of
tree automata the associated synthesis problem for $\omega$-regular 
specifications can be solved rather transparently. The idea is to use trees to 
model the interaction between input and output symbols. The directions of a 
tree model inputs while the symbols in the tree represent outputs. Similar to 
the arguments of the proof of Theorem \ref{thm:POMDPequivWDTA} a tree can 
therefore be understood as strategy and we want to design a \ac{PTA} such that 
a tree is only accepted if all paths are part of $\mathcal{L}_{\phi}$. This can 
be easily modelled (cp. \cite[Lemma 15]{AutoInfObj}) with
\begin{definition}[Synthesis \ac{PTA}]
  For a given \ac{DPA}
  \begin{equation*}
    \mathcal{P} = \tuple{Q, q_{0}, J\times I, \delta, \parity}
  \end{equation*}
  we define a \ac{PTA}
  \begin{equation*}
    \mathcal{A} = \tuple{Q, q_{0}, I, J, \Delta, \parity}
  \end{equation*}
  where $\Delta$ contains transtitions $\tuple{q, j, (p_{i})_{i\in I}}$ with
  $p_{i} = \delta(q, \tuple{j, i})$.
\end{definition}
Notably, $\mathcal{A}$ executes $\mathcal{P}$ on every path and therefore, if 
we choose $\mathcal{P}$ as the \ac{PTA} which precisely accepts 
$\mathcal{L}_{\phi}$, we deduce that $\mathcal{A}$ accepts only those trees 
that satisfy $\phi$ on all paths. This implies that $\mathcal{A}$ accepts those
strategies $t$ which satisfy $\phi$. Hence, constructing (see above) and 
deciding the emptiness (see Theorem \ref{thm:emptinessPTA}) for $\mathcal{A}$ 
yields the desired synthesis algorithm for $\omega$-regular specifications 
$\phi$ and gives
\begin{theorem}
  \cite[Theorem 21, Theorem 22]{AutoInfObj}
  The synthesis problem for $\omega$-regular specifications $\phi$ can be 
  decided.
\end{theorem}
This approach suits the game-theoretic interpretation of input- and 
output-player by identifying the input player with \pathfinder{} while the 
output player is assigned the role of \automaton{} and constructs a fitting 
tree.
\fxwarning{maybe elaborate on the calss of $\omega$-regular specifications 
(LTL, ...)}

Another approach is introduced in \cite{SynProbEnv} by considering environments
which do not act antagonistically but probabilistically, i.e. we assume that 
inputs are generated by a probabilistic process. This motivates the concept of 
$\epsilon$-environments. For an $\epsilon > 0$ such an environment generates an
input $i$ with a certain probability which is at any time bound from below by 
$\epsilon$. The form of the strategy that is meant to be synthesised is given
as a $J$-labeled $I$-tansition-system $\mathcal{T}$ of the form
\begin{equation*}
  \mathcal{T} = \tuple{S, s_{0}, \tau, \ell}.
\end{equation*}
Here $S$ is a set of states, $s_{0}$ an inital state and 
$\tau:S\times I\rightarrow S$ describes a deterministic transition function and
$\ell:S\rightarrow J$ a labelling of $S$. Semantically, $\mathcal{T}$ starts in 
$s_{0}$ and reacts to an input $i$ by moving to $z = \tau(s,i)$ and outputting 
$\ell(z)$. Naturally, such a transition system $\mathcal{T}$ models a strategy 
$t:I^{+}\rightarrow J$ by constructing for every 
$u = u_{1}\dots u_{n}\in I^{+}$ the unique sequence 
$s_{0}s_{1}\dots s_{n}\in S^{+}$ such that $s_{i+1} = \tau(s_{i}, u_{i+1}$ for 
$0\leq i<n$ and returning $\ell(s_{n})$. As mentioned in the introduction of 
Section \ref{subsec:ata} alternating tree automata can be used to run on the 
unfolding of such transition systems. Assuming a $\omega$-regular specification 
$\phi$ we may obtain a \ac{DPA} 
$\mathcal{P} = \tuple{Q, q_{0}, \tuple{J\times I}, \delta, \parity}$ accepting 
$\mathcal{l}_{\phi}$. We can attach $\mathcal{P}$ onto a given transition 
system $\mathcal{T}$ in a straightforward manner to obtain a transition system 
with an associated Parity-condition (but dropping the labelling)
\begin{equation*}
  \mathcal{G}_{\mathcal{P}}^{\mathcal{T}} = \tuple{S\times Q, 
    \tuple{s_{0}, q_{0}}, \tau', \parity'}
\end{equation*}
where $\tau'(\tuple{s, q}, i) = \tuple{\tau(s, i), 
\delta(q, \tuple{\ell(s), i})}$ and $\parity'(\tuple{s, q}) = \parity(q)$. For
this transition system and an $\epsilon$-environment $\mathcal{E}$ we can 
identify structural properties in $\mathcal{G}_{\mathcal{P}}^{\mathcal{T}}$ to
determine whether $\mathcal{T}$ satisfies $\phi$ almost-surely respectively 
positively. This structural property is connected with \ac{SCC}. These are 
subsets of nodes $G$ such that there is a path between every two nodes 
$u,v\in G$ (cp. \cite{Tarjan}). Additionally, a leaf-\ac{SCC} is an \ac{SCC} 
$S$ such that there is no other \ac{SCC} reachable from any $v\in S$ (cp. 
\cite[Bottom-\ac{SCC}]{ComplexProbVerification}). Intuitively, it is unlikely 
to not eventually end up in one of these leaf-\acp{SCC} since there is 
infinitely often the possibility to take a non-zero probability to move into 
these, hence we state
\begin{lemma}
  \cite[Lemma 1]{SynProbEnv}
  An $J$-labeled $I$-transition-system $\mathcal{T}$ almost-surely (positively)
  satisfies a specification $\phi$ if and only if the highest priority in all
  (some) reachable leaf-\ac{SCC} of $\mathcal{G}_{\mathcal{P}}^{\mathcal{T}}$
  is even, where $\mathcal{P}$ is a \ac{DPA} accepting $\mathcal{L}_{\phi}$.
\end{lemma}
This structual property can be checked by a \ac{APTA}. This again yields by
checking the emptiness of the obtained \ac{APTA} an algorithm for the synthesis
problem of almost-sure (respectively positive) satisfaction of an 
$\omega$-regular $\phi$. Therefore we obtain
\begin{theorem}
  \cite[Theorem 1]{SynProbEnv}
  Given a \ac{DPA} $\mathcal{P}$ there is a \ac{APTA} 
  $\mathcal{A}_{\mathcal{P}}$ ($\mathcal{O}_{\mathcal{P}}$) such that 
  $\mathcal{A}_{\mathcal{P}}$ ($\mathcal{O}_{\mathcal{P}}$) accepts a 
  $J$-labeled $I$-transition-system $\mathcal{T}$ if and only 
  if\footnote{The original theorem only states the if-direction but the 
  corresponding proof deals with the only if-direction as well.} $\mathcal{T}$ 
  satisfies almost-surely (positively) $\mathcal{P}$.

  If $\mathcal{P}$ has $n$ states and $c$ parities then 
  $\mathcal{A}_{\mathcal{P}}$ and $\mathcal{O}_{\mathcal{P}}$ have at most
  $n\cdot\lceil 2 + \frac{c}{2}\rceil + 1$ states.
  \label{thm:probenvsynthesis}
\end{theorem}
\begin{proof}[Sketch]
  The main argument for this proof is that the search for the leaf-\acp{SCC} 
  can be understood as a two-player-game. One player plays for acceptance 
  (called \acceptor{}) while the other tries to spoil the correctness of the 
  transition system (called \spoiler{}). We describe the case of 
  $\mathcal{A}_{\mathcal{P}}$ but the case $\mathcal{O}_{\mathcal{P}}$ can be 
  constructed analogously by exchanging the player role in the first phase of 
  the game. The game for $\mathcal{T}$ in the $\mathcal{A}_{\mathcal{P}}$ 
  operates in three phases:
  \begin{enumerate}
    \item \spoiler{} chooses one leaf-\ac{SCC} $S$,
    \item \acceptor{} chooses one priority $p$ of a node in $S$,
    \item \spoiler{} tries to find a higher priority than $p$ in $S$.
  \end{enumerate}
  The unrollment of $\mathcal{T}$ yields an $I$-ary $J$-tree. The states of 
  $\mathcal{A}_{\mathcal{P}}$ encode the current state of $\mathcal{P}$ and if
  the game is in the first, second or third phase. If the game is in the third
  phase the chosen priority is also stored in the state. While operating in the
  first phase $\mathcal{A}_{\mathcal{P}}$ dispatches in all directions 
  first-phase states, representing possible choices of \spoiler{} and also in
  one direction a second-phase state; that is a state which only dispatches a
  second phase state into one direction, representing the possible choices of
  \acceptor{}. This second-phase state may at any point transform to a 
  third-phase state and by this encodes the (even) priority of the current 
  state of $\mathcal{P}$ in the state of $\mathcal{A}_{\mathcal{P}}$. This 
  third-phase state again dispatches in all directions and as soon as one state 
  with a higher parity than the stored one appears, moves to a 
  sink-error-state. The first-phase states are given a parity of $0$, the 
  second-phase states of $1$ and the third-phase states of $2$. This allows 
  that for all choices of \spoiler{} (which constantly reproduce themselves) 
  only those that proceed to the second phase actually matter or for the 
  third-phase only the occurence of a higher parity. Since the parity of the 
  second-phase states is odd it is enforced that \acceptor{} eventually chooses 
  one parity to store. Note the similarity to the proof of Proposition
  \ref{prop:atanegation} in formulating the desired winning strategy of one 
  player as actual choices which may use the non-determinism of the automaton 
  while the enemy explores all possible moves. Also, we omit the precise 
  definition of $\mathcal{A}_{\mathcal{P}}$ here since its rather involved 
  technicalities do not translate well to the chosen formalism for alternating 
  automata of this paper.
\end{proof}

This allows us to solve the associated synthesis problem for $\omega$-regular
specifications and $\epsilon$-environments. In the following we consider 
environments that are modelled as \acp{MDP} or \acp{POMDP} and try to synthesis
for a given specification a strategy to almost-surely or positively satisfy 
this specification. This approach is conceptually different to 
$\epsilon$-environments since concrete probabilities are given and also the 
interaction of the actor with the environment is explicitly incorporated but 
can be understood as approach to annotate the transitions of $\mathcal{T}$
with the probabilities of the environment.

\section{Strategy Synthesis for \ac{POMDP}}
We move towards synthesis in environments which are explicitly modelled in 
terms of \acp{MDP} and \acp{POMDP}. Then we formalize the synthesis question 
with (cp. \cite{QualAnaPOMDP, SimpleStochasticParityGames, 
ComplexProbVerification})
\begin{definition}
  Given a \ac{POMDP} 
  \begin{equation*}
    \mathcal{M} = \tuple{S, s_{0}, A, \tuple{\tau_{a}}_{a\in A}, \sim}
  \end{equation*}
  and a specification $\phi\subseteq S^{\omega}$ such that 
  $\phi\in\mathcal{B}(S)$. The qualitative synthesis problem 
  $\tuple{M, \varphi}$ demands the computation of a strategie 
  $s:\interval{S}_{\sim}^{*}\rightarrow A$ such that $\mu_{s}(\varphi) = 1$.
\end{definition}
We already used results formulated for this question, namely Theorem 
\ref{thm:pomdpstratsynthesis} and Theorem \ref{thm:mpdstratsynthesis}. Again,
we focus on $\omega$-regular $\phi$ (in this case $\phi$ is itself a language
and is not translated to one). If we additional assume that 
$\size{\interval{s}_{\sim}} = 1$ for all $s\in S$, i.e. $\mathcal{M}$ 
degenerates to an \ac{MDP}, similar arguments as for $\epsilon$-environments 
above may be used. In analogy to 
\cite[Proposition 4.2.3.]{ComplexProbVerification} we deduce
\begin{lemma}
  For any \ac{POMDP} $\mathcal{M} = \tuple{S, s_{0}, A, 
  \tuple{\tau_{a}}_{a\in A}, \sim}$ and $\omega$-regular 
  $\phi\subseteq S^{\omega}$ there is an \ac{POMDP} $\mathcal{M'}$ with an 
  associated Parity-condition $\parity$ such that any strategy $s$ for 
  $\mathcal{M}$ induces a strategy $s'$ for $\mathcal{M'}$ such that 
  $\mu_{s}(\phi) = \mu_{s'}(\Acc(\parity))$ and vice versa.
  \label{lem:POMDPomegareg}
\end{lemma}
\begin{proof}
  For $\phi$ we can obtain a \ac{DPA} 
  $\mathcal{P} = \tuple{Q, S, q_{0}, \delta, \parity}$ which accepts $\phi$.
  The deterministic nature of $\mathcal{P}$ is used to obtain the result by
  constructing
  \begin{equation*}
    \mathcal{M'} = \tuple{S\times Q, \tuple{s_{0}, q_{0}}, A, 
    \tuple{\tau'_{a}}_{a\in A}, \sim', \parity'}
  \end{equation*}
  with
  \begin{align*}
    &\tau'_{a}(\tuple{s, q}, \tuple{z, p}) = \begin{cases}
      \tau_{a}(s, z)&\text{if }p = \delta(q,s),\\
      0&\text{otherwise,}
    \end{cases}\\
    &\parity'(\tuple{s,q}) = \parity(q)
    \text{ and }
    \tuple{s, q}\sim'\tuple{z, p}\text{ iff }s\sim z.
  \end{align*}
  Since $\sim'$ is defined in terms of the first element of every state, we may
  biject $\interval{s}_{\sim}$ with $\interval{\tuple{s, q}}_{\sim'}$ which 
  allows to translate strategies from $\mathcal{M}$ to $\mathcal{M'}$ and vice
  versa and additionally by definition of $\tau_{a}'$ for all $a\in A$ we 
  obtain for any strategy $s'$ for $\mathcal{M'}$ and its corresponding 
  strategy $s$ for $\mathcal{M}$ that
  \begin{equation*}
    \mu_{s'}(\cyl(\tuple{z_{0}, q_{0}}\dots\tuple{z_{n}, q_{n}})) = 
    \begin{cases}
      \mu_{s}(\cyl(z_{0}\dots z_{n}))&\text{for the unique }
        q_{0}\dots q_{n}\text{ of }\mathcal{P}\text{ on }z,\\
      0&\text{otherwise.}
    \end{cases}
  \end{equation*}
  We may deduce that the first component of all valid, i.e. not impossible, 
  plays in $\mathcal{M}'$ uniquely identifies the second component which allows 
  us to only consider the projection to those \enquote{synchronised} plays 
  which can easily bijected to plays in $\mathcal{M}$ and $s$ and $s'$ even 
  induce the same measure on this bijection. We conclude that
  \begin{equation*}
    \mu_{s}(\phi) = \mu_{s'}(\Acc(\parity)).
  \end{equation*}
\end{proof}

If we actually consider in the argument above that $\mathcal{M}$ is an \ac{MDP} 
then the product construction can be considered to be an \ac{MDP} as well since 
the uniqueness of the second component allows to use the same argument. 
Although the strategies space for $\mathcal{M'}$ are richer, we can only 
consider the space of possible plays which again corresponds uniquely to plays 
in $\mathcal{M}$. We obtain
\begin{corollary}
  For an initial \ac{MDP} $\mathcal{M}$ one obtains a corrresponding \ac{MDP} 
  $\mathcal{M}'$ with the same properties as stated in Lemma 
  \ref{lem:POMDPomegareg}.
\end{corollary}
Moreover, if we consider $\phi$ to be recognizable by a \ac{DBA} we obtain by
the same argument
\begin{corollary}
  For a \ac{DBA}-recognizable $\phi$ and a \ac{POMDP} (\ac{MDP}) $\mathcal{M}$
  the construction from Lemma \ref{lem:POMDPomegareg} yields an \ac{POMDP} 
  (\ac{MDP}) $\mathcal{M'}$ with an associated Büchi-condition.
  \label{cor:POMDPDBA}
\end{corollary}
Concludingly, if we consider that associated Büchi- or Parity-conditions on
\ac{POMDP} induce $\omega$-regular $\phi\subseteq S^{\omega}$ we conclude that
the two formulations are interchangeable. This allows to conclude that 
\ac{DBA}-recognizable $\phi$ allow for strategy synthesis in \acp{POMDP} (cp.
Theorem \ref{thm:pomdpstratsynthesis}) but for $\omega$-regular $\phi$ it is
undecideable to compute strategies for \acp{POMDP}.

The arguments to derive e.g. the decideability of \acp{MDP} with associated
Büchi-condition presented in \cite{ComplexProbVerification} do follow a similar
approach to $\epsilon$-environments above by identifying leaf-\acp{SCC} which
contain an accepting state. Naturally, these \acp{SCC} are dependent on the
strategies which renders the arguments more involved but conceptually the same
arguments hold, namely it is almost-sure to end up in an \ac{SCC} and - once 
arived - it is also almost-sure to see every state in this \ac{SCC} infinitely
often. \fxfatal{proceed here}


In the following we 
consider $\phi$ which are defined in terms of \acp{PBA}. This induces the 
following question:
\begin{definition}[PBA-Synthesis Question]
  Given a \ac{POMDP} $\mathcal{M}$ and a \ac{PBA} $\mathcal{P}$ with
  \begin{equation*}
    \mathcal{M} = \tuple{S, s_{0}, A, \tuple{\tau_{a}}_{a\in A}, \sim}
  \text{ and }
    \mathcal{P} = \tuple{Q, q_{0}, S, \delta, F}
  \end{equation*}
  exists a strategy $s:\interval{S}_{\sim}^{*}\rightarrow A$ such that 
  almost-all executions of $\mathcal{A}$ under $s$ are accepted by 
  $\mathcal{P}$?
  \label{def:synthesis}
\end{definition}
Considering a qualitative acceptance condition on $\mathcal{P}$ we can answer
this question positively with
\begin{theorem}[Qualitative PBA Synthesis]
  The synthesis question of an environment $\mathcal{M}$ and a specification
  provided as \ac{PBA} $\mathcal{A}$ with a qualitative acceptance measure
  can be decided.
  \label{thm:pbasynthesis}
\end{theorem}
Considering a certain restricted class of \acp{MDP}, namely those where for 
every state ever following state is defined by a probability distribution $B$
(regardless of the action chosen by the player), then this result can be 
derived as a corollary from the construction examined in Example 
\ref{ex:pwapbaallpaths}. Although the following construction bears structural
similarities with the one presented there, the arguments used here are a little 
more involved.

\begin{proof}
  We fix one strategy $s:\interval{S}_{\sim}^{*}\rightarrow A$ which induces a 
  probability space
  \begin{equation*}
    \tuple{S^{\omega},\mathcal{B}(S), \mu_{s}}.
  \end{equation*}
  Furthermore, for any given $\alpha\in S^{\omega}$ the stochastic process of
  $\mathcal{P}$ also yields a probability space
  \begin{equation*}
    \tuple{Q^{\omega},\mathcal{B}(Q), \mu_{\alpha}}.
  \end{equation*}
  The analytical complexity is rooted in the dependency of $\mu_{\alpha}$ from
  $\alpha$. We approach this dependency in terms of \emph{Markov-kernels}. We
  define (analogously to \cite[Definition 8.25]{Klenke})
  \begin{definition}[Markov-Kernel]
    For two measurable spaces $\tuple{\Omega_{1},\mathcal{F}_{1}}$ and 
    $\tuple{\Omega_{2},\mathcal{F}_{2}}$ a function
    \begin{equation*}
      K:\Omega_{1}\times\mathcal{F}_{2}\rightarrow\interval{0,1}
    \end{equation*}
    is called a Markov-kernel if
    \begin{enumerate}
      \item $K(\cdot, A)$ is measurable in $\mathcal{F_{1}}$ for all 
        $A\in\mathcal{F}_{2}$,
      \item $K(\omega, \cdot)$ is a probability measure on 
        $\tuple{\Omega_{2},\mathcal{F}_{2}}$ for every $\omega\in\Omega_{1}$.
    \end{enumerate}
  \end{definition}
  We define the following Markov-kernel
  \begin{equation*}
    K:S^{\omega}\times\mathcal{B}(Q)\rightarrow\interval{0,1}\text{ with }
      K(\alpha, A) = \mu_{\alpha}(A)
  \end{equation*}
  for the measurable spaces $\tuple{S^{\omega},\mathcal{B}(S)}$ and 
  $\tuple{Q^{\omega},\mathcal{B}(Q)}$. The central inside arises from Lemma 
  \ref{lem:almosteverywhere} which allows us to state
  \begin{equation*}
    \int_{S^{\omega}}K(\cdot, \Acc(F)) d\mu_{s} 
    = \int_{\alpha\in S^{\omega}}\mu_{\alpha}(\Acc(F))d\mu_{s} = 1 \text{ iff }
      \mu_{s}(K(\cdot,\Acc(F))^{-1}(1)) = 1.
  \end{equation*}
  This characterises the synthesis question from Definition \ref{def:synthesis}
  since $\mu_{s}(K(\cdot,\Acc(F))^{-1}(1))$ is the set of executions which are
  accepted by an almost-sure measure by $\mathcal{P}$. Notably, we can assure 
  this statement only under the premise that $K(\cdot, \Acc(F))$ is indeed 
  measurable. This is a direct consequence if $K$ is a Markov-kernel since 
  $\Acc(F)\in\mathcal{B}(Q)$. We address this condition with the following
  \begin{lemma}
    $K$ as defined above is a Markov-kernel.
  \end{lemma}
  \begin{proof}
    With $K(\alpha,\cdot) = \mu_{\alpha}$ it is trivial to state that 
    $K(\alpha,\cdot)$ defines a probability measure for 
    $\tuple{Q^{\omega},\mathcal{B}(Q)}$.

    \cite[Remark 8.26]{Klenke} states that it is sufficient to check the second 
    condition only for sets in a family $\mathcal{E}$ if $\mathcal{E}$ is 
    closed under intersection and contains a sequence of sets 
    $\tuple{E_{i}}_{i\in\mathbb{N}}$ such that $\cup_{i\in\mathbb{N}}E_{i}$ is
    equal to the ground-set. We observed that the cylindric sets 
    $A\in\mathcal{B}(Q)$, i.e. $A = \cyl(u)$ for some $u\in Q^{*}$, satisfy 
    this requirement, hence we restrict the following argument to those.
    Fundamentally, we observe that by definition of the transition 
    probabilities for a \ac{PBA} we obtain for any $u\in S^{n}$ and $\alpha, 
    \beta\in\cyl(u)$ that
    \begin{equation*}
      \mu_{\alpha}(\cyl(v)) = \mu_{\beta}(\cyl(v))\text{ for all }v\in Q^{n}.
    \end{equation*}
    This justifies the use of the notion $\mu_{u}$.
    Regarding measurability of function we introduce
    \begin{theorem}
      \cite[Theorem 9.2]{Bauer}
      For a measurable space $\tuple{\Omega, \mathcal{F}}$ and a function 
      $f:\Omega\rightarrow\interval{0,1}$ the $\mathcal{F}$-measurability of
      $f$ is equivalent to one of the following conditions
      \begin{enumerate}
        \item for all $a\in\interval{0,1}$ holds 
          $\set{p\in\Omega\mid f(p) < a}\in\mathcal{F}$,
        \item for all $a\in\interval{0,1}$ holds 
          $\set{p\in\Omega\mid f(p) \leq a}\in\mathcal{F}$,
        \item for all $a\in\interval{0,1}$ holds 
          $\set{p\in\Omega\mid f(p) > a}\in\mathcal{F}$,
        \item for all $a\in\interval{0,1}$ holds 
          $\set{p\in\Omega\mid f(p) \geq a}\in\mathcal{F}$.
      \end{enumerate}
    \end{theorem}
    Therefore the measurability of $K(\cdot, \cyl(u))$ is
    implied if for all $a\in\mathbb{R}$
    \begin{equation*}
      \set{\alpha\in S^{\omega}\mid K(\alpha, \cyl(v))\leq a}\in\mathcal{B}(S).
    \end{equation*}
    Observably,
    \begin{equation*}
      \set{\alpha\in S^{\omega}\mid K(\alpha, \cyl(v))\leq a} = 
        \bigcup\set{
          \cyl(u):u\in S^{\size{v}}\text{ and }\mu_{u}(\cyl(v))\leq a
        }
    \end{equation*}
    implies the necessary membership in $\mathcal{B}(S)$ since $S^{\size{v}}$ 
    is finite.
  \end{proof}
  Having this Markov-kernel $K$ allows us to use the following
  \begin{theorem}
    \cite[Korollar 14.23]{Klenke}
    For a probability space $\tuple{\Omega_{1}, \mathcal{F}_{1}, \mu}$, a
    measurable space $\tuple{\Omega_{2}, \mathcal{F}_{2}}$ and a Markov-kernel
    $K:\Omega_{1}\times\mathcal{F}_{2}$ there exists a unique probability 
    measure $\mu\otimes K$ on $\tuple{\Omega_{1}\times \Omega_{2}, 
    \mathcal{F}_{1}\times\mathcal{F}_{2}}$ with
    \begin{equation*}
      \mu\otimes K(A_{1}, A_{2}) = \int_{A_{1}} K(\omega, A_{2})d\mu
      \text{ for all }A_{1}\in\mathcal{F}_{1}, A_{2}\in\mathcal{F}_{2}.
    \end{equation*}
  \end{theorem}
  
  We proceed in defining an appropiate \ac{WDTA} to capture precisely those 
  strategies which satisfy the synthesis requirement. The concept mirrors the
  resulting \ac{WDTA} when we combine the path measure and the run measure for
  \acp{PWA} (see Example \ref{ex:pwapbaallpaths}) by build run-trees of a 
  \ac{PBA} along the executions. But it is actually not necessary to restrict
  ourselves to one execution step of the \ac{PBA} per direction. This 
  observation is essential for formulating this result for \acp{POMDP} rather 
  than for \acp{MDP} alone.
  \begin{definition}[Synthesis WDTA]
    For a \ac{POMDP} $\mathcal{M}$ and a \ac{PBA} $\mathcal{P}$ with
    \begin{equation*}
      \mathcal{M} = \tuple{S, A, \tuple{\tau_{a}}_{a\in A}, s_{0}, \sim}
      \text{ with }
      \mathcal{O} = \set{\interval{s}_{\sim}: s\in S}
      \text{ and }
      \mathcal{P} = \tuple{Q, S, \delta, q_{0}, F}
    \end{equation*} 
    we construct the choiceless \ac{WDTA}
    \begin{equation*}
      \mathcal{A}_{\mathcal{M}}^{\mathcal{P}} = \tuple{
        S\times Q, \tuple{s_{0}, q_{0}}, A, \mathcal{O}, \Delta, S\times F
      }.
    \end{equation*}
    The transitions in $\Delta$ are of the form
    \begin{equation*}
      \tuple{\tuple{s, q}, a, G_{\tuple{s,q}}^{a}}
      \text{ with }
      G_{\tuple{s,q}}^{a}(\tuple{z,p}, o) = \begin{cases}
        \tau_{a}(s, z)\cdot\delta(q, z, p)&\text{if }\interval{z}_{\sim} = o,\\
        0&\text{otherwise}.
      \end{cases}
    \end{equation*}
  \end{definition}
  Notably, again all trees for this \ac{WDTA} 
  $\mathcal{A}_{\mathcal{M}}^{\mathcal{P}}$ are valid strategies for the 
  \ac{POMDP} $\mathcal{M}$. We fix one tree (or strategie respectively) $s$. 
  For the unique run $r$ of $\mathcal{A}_{\mathcal{M}}^{\mathcal{P}}$ on $s$ we
  can observe (analogously to the construction for Theorem 
  \ref{thm:POMDPequivWDTA}) that the $S$ component of any state uniquely 
  correlates to one $\mathcal{O}$ component and by the definition of 
  $G_{\tuple{s, q}}^{a}$ this is the only direction weight can be passed to.
  Therefore, we restrict our arguments on the weighted part of the domain of
  $r$, namely $\tuple{S\times Q}^{*}$. By definition of the transitions we 
  obtain for any word 
  $\tuple{s_{0}, q_{0}}\dots\tuple{s_{n}, q_{n}}\in\tuple{S\times Q}^{*}$
  \begin{align*}
    \mu_{r}(\tuple{s_{1}, q_{1}}\dots\tuple{s_{n}, q_{n}}) &= 
      G^{s(\epsilon)}(\tuple{s_{1}, q_{1}})\cdot 
      \prod\limits_{i = 1}^{n-1}
        G^{s(\interval{s_{1}\dots s_{n}}_{\sim})}_{\tuple{s_{i}, q_{i}}}(
          s_{i+1}, q_{i+1})\\
    &= \delta(q_{0}, s_{1}, q_{1})\cdot\tau_{s(\epsilon)}(s_{0}, s_{1})
      \prod\limits_{i = 1}^{n-1}
        \delta(q_{i}, s_{i+1}, q_{i+1})\cdot
        \tau_{s(\interval{s_{1}\dots s_{n}}_{\sim})}(s_{i}, s_{i+1})\\
    &= \mu_{s_{1}\dots s_{n}}(\cyl(q_{0}\dots q_{n}))\cdot
      \mu_{s}(\cyl(s_{1}\dots s_{n}))\\
    &= \int_{\cyl(s_{1}\dots s_{n})}K(\cdot, \cyl(q_{0}\dots q_{n}))d\mu_{s}.
  \end{align*}
  This suggests by the uniqueness of $\mu_{s}\otimes K$ and 
  $\mu_{r}$ that these coincide (under renaming of the arguments). It is left
  to examine this renaming for $\mu_{r}$ since $\mu_{r}$ is defined on 
  $\mathcal{B}(S\times Q)$ while $\mu_{s}\otimes K$ is defined on 
  $\mathcal{B}(S)\times\mathcal{B}(Q)$. Not suprisingly, we have
  \begin{lemma}
    $\mathcal{B}(S\times Q)$ and $\mathcal{B}(S)\times\mathcal{B}(Q)$ are 
    equivalent up to renaming.
  \end{lemma}
  \begin{proof}
    From Theorem \ref{thm:productgen} we know that 
    $\mathcal{B}(S)\times\mathcal{B}(Q)$ is generated by $\cyl(u)\times\cyl(v)$ 
    for all $u\in S^{*}$ and $v\in Q^{*}$. We show that we may 
    \enquote{balance} these generating sets (cp. Lemma \ref{lem:balancedruns}). 
    W.l.o.g. we assume $\size{u} < \size{v}$. From $m = \size{v} - \size{u}$ we 
    generate the set $E = \set{u\cdot y:y\in S^{m}}$. Since $S$ is finite so is 
    $E$ and we may use the generating sets $\cyl(u')\times\cyl(v)$ for all 
    $u'\in E$ to obtain the same set as $\cyl(u)\times\cyl(v)$. Hence, we may 
    restrict our generating sets to balanced cylindric sets. We may biject 
    tuple $\tuple{s_{1}\dots s_{n}, q_{1}\dots q_{n}}$ to 
    $\tuple{s_{1}, q_{1}}\dots \tuple{s_{n}, q_{n}}$ (even up to infinity) and
    obtain a bijection between the generating sets for $\mathcal{B}(S\times Q)$
    and $\mathcal{B}(S)\times\mathcal{B}(Q)$ which survives the construction of
    all elements in the algebra. This allows us to transform the both algebras
    into each other and additionally shows that $\mu_{s}\otimes K$ indeed
    coincides with $\mu_{r}$.
  \end{proof}
  Concludingly, we can observe for the acceptance meausure of 
  $\mathcal{A}_{\mathcal{M}}^{\mathcal{P}}$ 
  \begin{equation*}
    \mu_{r}(\Acc\interval{Q}(S\times F))  
    = \mu_{s}\otimes K(S^{\omega}, \Acc(F))
    = \int_{\alpha\in S^{\omega}}\mu_{\alpha}(\Acc(F)) d\mu_{s}.
  \end{equation*}
  This allows us to state that $\mathcal{A}_{\mathcal{M}}^{\mathcal{P}}$ 
  accepts $s$ if and only if $s$ is a solution to the synthesis question for 
  the \ac{POMDP} $\mathcal{M}$ and the \ac{PBA} $\mathcal{P}$. We may use
  Corollary \ref{cor:emptiness} to compute this $s$.
\end{proof}
Although inspired by the arguments of Theorem \ref{thm:}, the usage of 
\acp{WDTA} is not essential for the argument above. Equivalently Lemma 
\ref{lem:POMDPomegareg} can be extended towards $\phi$ which
are defineable by almost-surely accepting \acp{PBA} by the natural product 
construction: we consider a \ac{POMDP} $\mathcal{M}$ and a \ac{PBA} 
$\mathcal{P}$ with
\begin{equation*}
  \mathcal{M} = \tuple{S, s_{0}, A, \tuple{\tau_{a}}_{a\in A}, \sim}
  \text{ and }
  \mathcal{P} = \tuple{Q, q_{0}, S, \delta, F}
\end{equation*}
and construct a product \ac{POMDP}
\begin{equation*}
  \mathcal{M}\otimes\mathcal{P} = \tuple{S\times Q, \tuple{s_{0}, q_{0}}, A,
    \tuple{\tau'_{a}}_{a\in A}, \sim', F' = S\times F}
\end{equation*}
with
\begin{equation*}
  \tau'_{a}(\tuple{s, q}, \tuple{z, p}) = \tau_{a}(s, z)\cdot\delta(q, s, p)
  \text{ and }
  \tuple{s, q}\sim'\tuple{z, p}\text{ iff }s\sim z.
\end{equation*}
The correctness argument is analogously to the argument which involve 
\acp{WDTA} above. In this setup \acp{MDP} are also transformed to \ac{POMDP} 
since the construction needs to hide the current state of $\mathcal{P}$ and we
do not have a determinism argument as before since \acp{PBA} have in general
multiple following states. On the other hand, since \ac{PBA} capture \ac{DBA}
by setting every transition function to a Dirac measure on $Q$, this indeed is
a generalisation of Corollary \ref{cor:POMDPDBA}.
